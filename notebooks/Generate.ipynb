{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0356278",
   "metadata": {},
   "source": [
    "# Documentation Generator with Gen AI\n",
    "\n",
    "## Overview\n",
    "This documentation generator leverages Generative AI to automatically create feature descriptions from product module specifications, supporting multiple documentation standards.\n",
    "\n",
    "## Features\n",
    "\n",
    "### Input Processing\n",
    "- **Module Specification Parsing**: Consumes various formats of product module specifications (JSON, YAML, XML, Word)\n",
    "- **Metadata Extraction**: Automatically extracts key information like module names, functions, parameters, and dependencies\n",
    "- **Validation**: Ensures input specifications meet required schema standards\n",
    "\n",
    "### AI-Powered Generation\n",
    "- **Natural Language Processing**: Uses advanced NLP models to understand technical specifications\n",
    "- **Context-Aware Generation**: Maintains consistency across related modules and features\n",
    "- **Multi-Standard Support**: Generates documentation compliant with various standards (API docs, user manuals, technical specs)\n",
    "- **Language rules**: Follows the language rules specified for technical documentation\n",
    "\n",
    "### Output Formats\n",
    "- **Markdown Documentation**: Clean, readable format for web and version control\n",
    "- **HTML Reports**: Styled documentation with navigation and search capabilities\n",
    "- **PDF Generation**: Professional documentation for distribution\n",
    "- **API Documentation**: Interactive documentation with examples and testing capabilities\n",
    "\n",
    "### Supported Standards\n",
    "- **OpenAPI/Swagger**: RESTful API documentation\n",
    "- **JSDoc**: JavaScript code documentation\n",
    "- **Sphinx**: Python project documentation\n",
    "- **GitBook**: Collaborative documentation platform\n",
    "- **Confluence**: Enterprise wiki format\n",
    "\n",
    "## Usage Workflow\n",
    "1. **Input**: Feed product module specifications into the system\n",
    "2. **Processing**: AI analyzes and understands the technical content\n",
    "3. **Generation**: Creates human-readable feature descriptions\n",
    "4. **Output**: Delivers documentation in specified standard format(s)\n",
    "\n",
    "## Benefits\n",
    "- **Consistency**: Uniform documentation style across all modules\n",
    "- **Efficiency**: Automated generation saves manual documentation time\n",
    "- **Accuracy**: Reduces human error in technical documentation\n",
    "- **Scalability**: Handles large codebases and complex product suites\n",
    "- **Maintenance**: Easy updates when specifications change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7776397",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU python-docx Pillow pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2979eb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def search_word_files(folder_path):\n",
    "    \"\"\"\n",
    "    Recursively search for Word files (.doc, .docx) in a folder and its subdirectories.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder to search in\n",
    "        \n",
    "    Returns:\n",
    "        list: List of full paths to Word files found\n",
    "    \"\"\"\n",
    "    word_files = []\n",
    "    word_extensions = ('.doc', '.docx')\n",
    "    \n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(word_extensions):\n",
    "                full_path = os.path.join(root, file)\n",
    "                word_files.append(full_path)\n",
    "    \n",
    "    return word_files\n",
    "\n",
    "'''\n",
    "# Example usage\n",
    "folder_to_search = \"./pmds\" \n",
    "word_documents = search_word_files(folder_to_search)\n",
    "\n",
    "print(f\"Found {len(word_documents)} Word files:\")\n",
    "for doc in word_documents:\n",
    "    print(doc)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004f5645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ParagraphStyle('Title') id: 131002037580240\n",
      "_ParagraphStyle('Title') id: 131002037663072\n",
      "_ParagraphStyle('Title') id: 131002035118944\n",
      "_ParagraphStyle('Subtitle') id: 131002036837968\n",
      "_ParagraphStyle('Subtitle') id: 131002037571312\n",
      "_ParagraphStyle('Subtitle') id: 131002038671680\n",
      "_ParagraphStyle('Subtitle') id: 131002037599504\n",
      "_ParagraphStyle('Subtitle') id: 131002037664944\n",
      "_ParagraphStyle('Subtitle') id: 131002035213408\n",
      "_ParagraphStyle('Subtitle') id: 131002035356304\n",
      "_ParagraphStyle('Subtitle') id: 131002037598064\n",
      "_ParagraphStyle('Subtitle') id: 131002035993056\n",
      "_ParagraphStyle('Subtitle') id: 131002035117936\n"
     ]
    }
   ],
   "source": [
    "from docx import Document as DocxDocument\n",
    "from langchain_core.document_loaders import BaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from abc import ABC\n",
    "\n",
    "class DocxLoader(BaseLoader, ABC):\n",
    "    \"\"\"Loader that uses python-docx to load .docx files.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str):\n",
    "        \"\"\"Initialize with file path.\"\"\"\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Load the document and return its text content.\"\"\"\n",
    "        doc = DocxDocument(self.file_path)\n",
    "        full_text = []\n",
    "        for para in doc.paragraphs:\n",
    "            full_text.append(para.text)\n",
    "        text = \"\\n\".join(full_text)\n",
    "        metadata = {\"source\": self.file_path}\n",
    "        return [Document(page_content=text, metadata=metadata)]\n",
    "\n",
    "test_doc = DocxDocument(\"./pmds/PRISM.USRMGMT_User_Management/PMD_PRISM_USRMGMT_User_Management.docx\")\n",
    "control = True\n",
    "i = 0\n",
    "\n",
    "while control: \n",
    "    paragraph = test_doc.paragraphs[i]\n",
    "    print(paragraph.text, paragraph.style.name)\n",
    "    i += 1\n",
    "    if i >= len(test_doc.paragraphs):\n",
    "        break\n",
    "    control = bool(int(input(\"Continue (0, 1)?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb2cf135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./pmds/PRISM.USRMGMT_User_Management/PMD_PRISM_USRMGMT_User_Management.docx\n",
      "\n",
      "Document Analysis:\n",
      "Title: History\n",
      "Sections: 72\n",
      "Paragraphs: 287\n",
      "Images: 2\n",
      "Tables: 14\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "import pytesseract\n",
    "\n",
    "word_documents = [\"./pmds/PRISM.USRMGMT_User_Management/PMD_PRISM_USRMGMT_User_Management.docx\"]\n",
    "\n",
    "def extract_document_content(doc_path):\n",
    "    \"\"\"\n",
    "    Extract and semantically separate content from a Word document.\n",
    "    Identifies images and processes them with OCR or tagging.\n",
    "    \n",
    "    Args:\n",
    "        doc_path (str): Path to the Word document\n",
    "        \n",
    "    Returns:\n",
    "        dict: Structured content with text, images, and metadata\n",
    "    \"\"\"\n",
    "    doc = Document(doc_path)\n",
    "    content = {\n",
    "        'metadata': {\n",
    "            'file_path': doc_path,\n",
    "            'title': '',\n",
    "            'paragraphs_count': 0,\n",
    "            'images_count': 0\n",
    "        },\n",
    "        'sections': [],\n",
    "        'images': [],\n",
    "        'tables': []\n",
    "    }\n",
    "    \n",
    "    current_section = {'title': '', 'content': []}\n",
    "    \n",
    "    for para in doc.paragraphs:\n",
    "        # Check if paragraph is a heading\n",
    "        if para.style.name.startswith('Heading'):\n",
    "            # Save previous section if it has content\n",
    "            if current_section['content']:\n",
    "                content['sections'].append(current_section)\n",
    "            \n",
    "            # Start new section\n",
    "            current_section = {\n",
    "                'title': para.text.strip(),\n",
    "                'level': para.style.name,\n",
    "                'content': []\n",
    "            }\n",
    "            \n",
    "            # Set document title from first heading\n",
    "            if not content['metadata']['title'] and para.text.strip():\n",
    "                content['metadata']['title'] = para.text.strip()\n",
    "        else:\n",
    "            # Add paragraph content\n",
    "            if para.text.strip():\n",
    "                current_section['content'].append({\n",
    "                    'type': 'paragraph',\n",
    "                    'text': para.text.strip(),\n",
    "                    'style': para.style.name\n",
    "                })\n",
    "        \n",
    "        # Check for images in paragraph\n",
    "        for run in para.runs:\n",
    "            if run.element.xpath('.//a:blip'):\n",
    "                content['metadata']['images_count'] += 1\n",
    "                content['images'].append({\n",
    "                    'index': content['metadata']['images_count'],\n",
    "                    'paragraph_text': para.text.strip(),\n",
    "                    'type': 'embedded_image'\n",
    "                })\n",
    "    \n",
    "    # Add final section\n",
    "    if current_section['content']:\n",
    "        content['sections'].append(current_section)\n",
    "    \n",
    "    # Process tables\n",
    "    for i, table in enumerate(doc.tables):\n",
    "        table_data = []\n",
    "        for row in table.rows:\n",
    "            row_data = []\n",
    "            for cell in row.cells:\n",
    "                row_data.append(cell.text.strip())\n",
    "            table_data.append(row_data)\n",
    "        \n",
    "        content['tables'].append({\n",
    "            'index': i + 1,\n",
    "            'data': table_data,\n",
    "            'rows': len(table_data),\n",
    "            'columns': len(table_data[0]) if table_data else 0\n",
    "        })\n",
    "    \n",
    "    content['metadata']['paragraphs_count'] = len([s for section in content['sections'] for s in section['content']])\n",
    "    \n",
    "    return content\n",
    "\n",
    "def process_images_with_ocr(content):\n",
    "    \"\"\"\n",
    "    Process images in the document with OCR.\n",
    "    Note: This is a placeholder implementation as extracting actual image data from docx requires additional handling.\n",
    "    \"\"\"\n",
    "    for img in content['images']:\n",
    "        img['ocr_text'] = \"OCR processing would be implemented here\"\n",
    "        img['tags'] = [\"technical_diagram\", \"flowchart\", \"screenshot\"]  # Example tags\n",
    "    \n",
    "    return content\n",
    "\n",
    "# Process the first Word document\n",
    "if word_documents:\n",
    "    first_doc = word_documents[0]\n",
    "    print(f\"Processing: {first_doc}\")\n",
    "    \n",
    "    extracted_content = extract_document_content(first_doc)\n",
    "    # processed_content = process_images_with_ocr(extracted_content)\n",
    "    \n",
    "    print(f\"\\nDocument Analysis:\")\n",
    "    print(f\"Title: {extracted_content['metadata']['title']}\")\n",
    "    print(f\"Sections: {len(extracted_content['sections'])}\")\n",
    "    print(f\"Paragraphs: {extracted_content['metadata']['paragraphs_count']}\")\n",
    "    print(f\"Images: {extracted_content['metadata']['images_count']}\")\n",
    "    print(f\"Tables: {len(extracted_content['tables'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87c4b5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'paragraph',\n",
       "  'text': 'Secret - Sensitive information, such as passwords or cryptographic keys, that is known only to authorized users or systems',\n",
       "  'style': 'List Paragraph'},\n",
       " {'type': 'paragraph',\n",
       "  'text': \"Credential - Information used to verify an entity's identity, often in the form of a username and password, security token or by other means\",\n",
       "  'style': 'List Paragraph'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_content[\"sections\"][4][\"content\"][-3:-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
